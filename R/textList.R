#' Text List
#'
#' Holds variables for use in providing text to the Shiny app
#'
#'

introText <- 'Welcome to the teaching package! In this R package, you will learn the various methods for creating different types of visualization which includes graphs that are in one and two dimensions, and other visualization techniques. In addition, this package will help you learn the ideas of kernel techniques and implementations, as well as principal component analysis.'

single1 <- 'Let’s start with the basic one dimensional graph. One basic example of a single dimensional visualization you can create in R would be a histogram. You’re given the dataset of the fire incidents count in San Diego. Let’s say that you want to make a histogram. To create a histogram in R, you use the “hist()” function. One important aspect of this function is the “break” parameter, where you can set it equal to either “Sturges,” “Scott,” or “Freedman-Diaconis.”'
single2 <- 'Here is how the three main parameters for “break” are implemented:'
single3 <- 'Sturges is the default for the break argument, where it assumes the distribution to be Gaussian. This method tends to oversmooth more than what is desired. Oversmoothing occurs when there are less bins which tend to be wider and result in too much detail per bin. With this, one may miss important details within the dataset. In addition, oversmoothing also creates a high bias.  In a standard normal distribution with sample size greater than 200, Sturges will typically have less bins than Scott or Freedman-Diaconis. The number of bins doesn’t just depend on the distribution, but the sample size as well.'
single4 <- 'Scott method minimizes the integrated MSE bin width.'
single5 <- 'Freedman-Diaconis method is similar to the Scott method, except it minimizes the MSE while also doubling the Interquartile range for determining the bin width. The estimator is proportional to the IQR and thus less sensitive (when compared to sample standard deviation) to outliers within the data.'
single6 <- 'The best method to use out of the three depends on the sample size and the distribution type of the dataset. Generally, you would prefer the method with the most appropriate number of bins, as you want your visualization to be as detailed as possible. This is a main idea to take into account when choosing one of these three methods.'
single7 <- 'Try it for yourself. Use the "hist()" function to visualize “problem_count”, and set the break to Sturges, then to Scott, then to FD and compare the differences for the fire incidents dataset.'
single8 <- 'When you have a histogram that has many bins, let’s say you want to calculate the probability density of the bin within a certain interval. We do this through the Kernel density estimation, which measures the area of a certain chunk of a histogram. To do this, we use the “density()” function which brings up the chart of the bandwidth value, as well as the five number summary for the variables.'
single9 <- 'The “Kernel” parameter also needs to be defined. The choices for types of kernels which determine the shape of the curve include: rectangular, triangular, or gaussian. The default method of the “density()” function is the Gaussian distribution. Normally, the best curve to pick out of the three would be the one that appears the smoothest out of all the curves. But for our case, we will be displaying the curves of all three of these in one plot and you can see how they all compare. Use the “lines()” function to plot the “problem_counts” using all three options for kernels on a graph.'

double1 <- 'There are several ways you can plot a 2d graph. Some of these include the “hist2d()” function from the gplots package, the “hexbin()” function from the hexbin package, the “contour()” function, and many others.'
double2 <- 'For creating visualizations in 2D, let’s use the hexbin package to plot a 2D histogram using the “hexbin()” function. To do this, you need to call the “hexbin()” function within the “plot()” function. When analyzing your hexbin visualization, notice that the darker regions correspond to higher density areas and vice versa. In addition, notice you are able to change the number of bins using the “xbins” parameter which could help you better analyze your data.'
double3 <- 'Another visualization technique you can use is implemented using the “image()” function to display a “kde2d()” density function. The “kde2D()” function from the MASS package will store the variable and then call it with the “image()” function.'
double4 <- 'Remember that while you are creating the “kde2D()” function, you want to determine the bandwidth value for the x and y axis as well. Within the MASS package, you can use the “bandwidth.nrd()” function to define the bandwidths in each direction of the variables. Try this for yourself using “month_response” as your x variable, and “problem_count” as your y variable and see what you get. Then, apply your results to your “kde2D()” visualization plot.'
double5 <- 'It is worth noting how visually similar the “image()” function and the “kde2D()” function are when creating a 2 dimensional graph when trying to understand what it tells us about the problem in the context of the dataset.'

corr1 <- 'Within R, there are various types of visualizations that measure the correlation between two variables. These include the scatterplot matrix which plots the correlation between two variables in many combinations in the form of a matrix, and the corrplot which is the visualization of all the pairwise sample correlations in a dataframe.'
corr2 <- 'Try to create a scatterplot matrix of all quantitative variables using the “pairs()” function in R. Once created, the variables being analyzed will appear on the diagonal. To read this, the row is the x-axis and the column is the y-axis for each individual scatterplot within the matrix. The more linear the the pattern of the plot between the two variables, the more correlated they are and vice versa.'
corr3 <- 'Try to create a correlation matrix plot, using the “corrplot” package to see the relationship between each of the variables in a dataset. To do this you need to use the “cor()” function, with the parameters of your dataset stored in a variable, then simply call the variable using “corrplot(variable)”. The larger and darker the circle, the larger the correlation, and vice versa. The color scale on the right will also help you determine whether or not there is a positive or negative correlation.'
corr4 <- 'Ask yourself, what do the correlations between two variables mean in the grand scheme of things, and specify what they mean in the context of the fire incident dataset.'

prin1 <- 'The principal component analysis (PCA) is a dimension reduction method of visualizing correlated variables in a dimensional space for exploring multivariate data. The PCA is used for combining variables from a large dataset and analyzing the characterization between each of these two variables. Aka, it is asking what the high or low PCA values tell us between two different variables.'
prin2 <- 'Before starting your analysis, always make sure to standardize/scale your variables in order to have a common variance. After that, we can use one of two functions to do the PCA: “princomp()” or “prcomp()”. Although both will compute the PC’s, “prcomp()” is generally preferred. While “princomp()” uses a spectral decomposition approach, “prcomp()” uses a SVD approach. Notice that both use the parameter “x” to indicate which numerical matrix or data frame is being analyzed. However, the difference comes in other parameters: “prcomp()” uses the “scale” parameter to indicate whether or not the variables were scaled before analysis while “princomp()” uses the “cor” parameter. “princomp()” also has a “scores” parameter to indicate whether or not the coordinates are calculated. The overall results, however, are nearly identical.'
prin3 <- 'After creating a table of PC’s, notice that for each column represented in each of the principal components, there is a value that is associated with each of the variables. Keep in mind that PC1 is the one-dimensional representation of the data that captures the largest amount of variability. There are positive and negative values associated with each variable, which indicate the direction of the principal component. What matters in the table is how close or far the PC value is from zero. If it is the furthest away from zero, this means that it has a high PC value and thus, the associated variable has the most dominant impact on the situation, and vice versa.'
prin4 <- 'Let’s say you want to make a scree plot of the principal component analysis. To do this, you want to use the “cov()” function within the “eigen()” function and store that in a variable. Then, use that variable as a parameter to plot the PCA, while dividing the “cumsum()” function by the “sum()” function as a parameter for the scree plot. Don’t forget to chain the eigen variable with all the values for the scree plot as well.'
prin5 <- 'When you get a scree plot, if you want to find the number of PC’s that capture a certain amount of variability in the data, you can use the “abline()” function with your desired percentage, which will create a boundary of the y-axis in the screeplot. A common percentage used for this is 90 percent. So any number of points that’s below the PC line in the graph is the number of PCs you would have. Also take into account if there is a PC that is above the line, but also close to it, it’s considered in the count for the number of PC’s that captures the variability in the data. Try this for yourself with a 90 percent “abline()” and see how many PC values you get.'

outroText <- 'And that’s all of how visualization, the kernel implication, and the PCA works within this package. We hope you found this package useful, learned a lot about these ideas, and are able to implement them in your future coding career!'
